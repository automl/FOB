optimizer:
  name: adamcpr
  learning_rate: 1.e-3
  beta1: 0.9
  beta2: 0.999
  epsilon: 1.e-8
  kappa_init_param: 1.0
  kappa_init_method: warm_start_factor
  reg_function: l2
  kappa_update: 1.0
  regularize_embedding: false
  reg_by_lr: false
  reg_step_size: 200
  reg_ema_decay: 0.9
