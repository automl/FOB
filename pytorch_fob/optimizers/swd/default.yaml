optimizer:
  name: adamw_baseline
  learning_rate: 1.e-3
  beta1: 0.9
  beta2: 0.999
  weight_decay: 1.e-4
  epsilon: 1.e-8
